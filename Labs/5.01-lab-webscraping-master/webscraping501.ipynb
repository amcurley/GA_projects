{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries here\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a soup object from the home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://pages.git.generalassemb.ly/rldaggie/for-scraping/'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the home page soup for every restaurant\n",
    "\n",
    "Note: Your best bet is to create a list of dictionaries, one for each restaurant. Each dictionary contains the restaurant's name and path from the `href`. The result of your scrape should look something like this:\n",
    "\n",
    "```python\n",
    "restaurants = [\n",
    "    {'name': 'A&W Restaurants', 'href': 'restaurants/1.html'}, \n",
    "    {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = soup.find('table', {'id': 'restaurants'})\n",
    "type(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant = []\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    \n",
    "    \n",
    "    restaurants = {}\n",
    "    \n",
    "    restaurants['restaurant'] = row.find('a').text\n",
    "    restaurants['link'] = row.find('a').attrs['href']\n",
    "    \n",
    "    restaurant.append(restaurants)\n",
    "    \n",
    "# pd.DataFrame(restaurant)\n",
    "len(restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A&W Restaurants\n",
      "Applebee's\n",
      "Arby's\n",
      "Atlanta Bread Company\n",
      "Bojangle's Famous Chicken 'n Biscuits\n",
      "Buffalo Wild Wings\n",
      "Burger King\n",
      "Captain D's\n",
      "Carl's Jr.\n",
      "Charley's Grilled Subs\n",
      "Chick-fil-A\n",
      "Chili's\n",
      "Chipotle Mexican Grill\n",
      "Church's\n",
      "Corner Bakery Cafe\n",
      "Dairy Queen\n",
      "Denny's\n",
      "El Pollo Loco\n",
      "FATZ\n",
      "Fazoli's\n",
      "Five Guys Burgers and Fries\n",
      "Golden Chick\n",
      "Hardee's\n",
      "IHOP\n",
      "In-N-Out Burger\n",
      "Jack in the Box\n",
      "Jimmy Johns\n",
      "Joe's Crab Shack\n",
      "KFC\n",
      "McDonald's\n",
      "O'Charley's\n",
      "Olive Garden\n",
      "Outback Steakhouse\n",
      "Panda Express\n",
      "Panera Bread\n",
      "Popeye's\n",
      "Quiznos\n",
      "Red Robin Gourmet Burgers\n",
      "Romano's Macaroni Grill\n",
      "Ruby Tuesday\n",
      "Subway\n",
      "Taco Bell\n",
      "Taco Bueno\n",
      "Wendy's\n"
     ]
    }
   ],
   "source": [
    "for n in restaurant:\n",
    "    print(n['restaurant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using the `href`, scrape each restaurant's page and create a single list of food dictionaries.\n",
    "\n",
    "Your list of foods should look something like this:\n",
    "```python\n",
    "foods = [\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Note**: Remove extra white space from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = []\n",
    "\n",
    "for res in restaurant:\n",
    "    url = f'https://pages.git.generalassemb.ly/rldaggie/for-scraping/'+res['link']\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.content, 'lxml')\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        food = {}\n",
    "        \n",
    "        food['calories'] = row.find_all('td')[2].text\n",
    "        food['carbs'] = row.find_all('td')[4].text\n",
    "        food['category'] = row.find_all('td')[1].text\n",
    "        food['fat'] = row.find_all('td')[3].text\n",
    "        food['name'] = row.find('td').text\n",
    "        food['restaurant'] = res['restaurant']\n",
    "        \n",
    "        foods.append(food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a pandas DataFrame from your list of foods\n",
    "\n",
    "**Note**: Your DataFrame should have 5,131 rows. Please output the number of rows in your DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5131, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows does your dataframe have?\n",
    "# 5131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export to csv\n",
    "\n",
    "**Note:** Don't export the index column from your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('food_list.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
