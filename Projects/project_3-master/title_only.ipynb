{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup       \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ai = pd.read_csv('../project_3-master/data/data_ai.csv')\n",
    "data_ml = pd.read_csv('../project_3-master/data/data_ml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>How MSMEs Manipulates Marketing Strategies for...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>Digital marketing trends that Paves the Way of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>How to Boost Your Team’s Performance and Produ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>How is Artificial Intelligence Bringing Pivota...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>Very promising and developing project. Modern ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit                                              title  \\\n",
       "0  ArtificialInteligence  How MSMEs Manipulates Marketing Strategies for...   \n",
       "1  ArtificialInteligence  Digital marketing trends that Paves the Way of...   \n",
       "2  ArtificialInteligence  How to Boost Your Team’s Performance and Produ...   \n",
       "3  ArtificialInteligence  How is Artificial Intelligence Bringing Pivota...   \n",
       "4  ArtificialInteligence  Very promising and developing project. Modern ...   \n",
       "\n",
       "  selftext  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ai.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Merge the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_ai.append(data_ml).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>How MSMEs Manipulates Marketing Strategies for...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>Digital marketing trends that Paves the Way of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>How to Boost Your Team’s Performance and Produ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>How is Artificial Intelligence Bringing Pivota...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ArtificialInteligence</td>\n",
       "      <td>Very promising and developing project. Modern ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit                                              title  \\\n",
       "0  ArtificialInteligence  How MSMEs Manipulates Marketing Strategies for...   \n",
       "1  ArtificialInteligence  Digital marketing trends that Paves the Way of...   \n",
       "2  ArtificialInteligence  How to Boost Your Team’s Performance and Produ...   \n",
       "3  ArtificialInteligence  How is Artificial Intelligence Bringing Pivota...   \n",
       "4  ArtificialInteligence  Very promising and developing project. Modern ...   \n",
       "\n",
       "  selftext  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit        0\n",
       "title            0\n",
       "selftext     13652\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see what a title might look like:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How MSMEs Manipulates Marketing Strategies for Success https://onpassive.pt/how-msmes-manipulates-marketing-strategies-for-success/?feed_id=14768&amp;_unique_id=5f3a974c04f3e'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['title']]\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How MSMEs Manipulates Marketing Strategies for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  How MSMEs Manipulates Marketing Strategies for..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27986, 1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27986,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20989, 1)\n",
      "(6997, 1)\n",
      "(20989,)\n",
      "(6997,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8909     How Artificial Intelligence Has Transformed Ba...\n",
       "18689                                      NLP Theoretical\n",
       "20762            [D] Help me choosing the best valued GPU.\n",
       "18314    Machine Learning in Materials Modeling -- Fund...\n",
       "16651    Perceptron Learning Algorithm Explained in Detail\n",
       "                               ...                        \n",
       "17787                  MLPs to Find Extrema of Functionals\n",
       "21793    [R] Speeding Up Neural Network Training with D...\n",
       "7063           Filter Out Your Data With Today's Simple AI\n",
       "16868    [R] Style-Controllable Speech-Driven Gesture S...\n",
       "22904    Is there a Python version of Dr. Koller's Prob...\n",
       "Name: title, Length: 20989, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    review_text = raw_review\n",
    "#     BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stop words to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Remove stop words.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27986 titles.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe size.\n",
    "total_titles = df.shape[0]\n",
    "print(f'There are {total_titles} titles.')\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews.\n",
    "clean_train_titles = []\n",
    "clean_test_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set for titles...\n",
      "Review 1000 of 27986.\n",
      "Review 2000 of 27986.\n",
      "Review 3000 of 27986.\n",
      "Review 4000 of 27986.\n",
      "Review 5000 of 27986.\n",
      "Review 6000 of 27986.\n",
      "Review 7000 of 27986.\n",
      "Review 8000 of 27986.\n",
      "Review 9000 of 27986.\n",
      "Review 10000 of 27986.\n",
      "Review 11000 of 27986.\n",
      "Review 12000 of 27986.\n",
      "Review 13000 of 27986.\n",
      "Review 14000 of 27986.\n",
      "Review 15000 of 27986.\n",
      "Review 16000 of 27986.\n",
      "Review 17000 of 27986.\n",
      "Review 18000 of 27986.\n",
      "Review 19000 of 27986.\n",
      "Review 20000 of 27986.\n",
      "Cleaning and parsing the testing set for titles...\n",
      "Review 21000 of 27986.\n",
      "Review 22000 of 27986.\n",
      "Review 23000 of 27986.\n",
      "Review 24000 of 27986.\n",
      "Review 25000 of 27986.\n",
      "Review 26000 of 27986.\n",
      "Review 27000 of 27986.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the training set for titles...\")\n",
    "\n",
    "j = 0\n",
    "\n",
    "for train_title in X_train['title']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_train_titles.append(review_to_words(train_title))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_titles}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "\n",
    "print(\"Cleaning and parsing the testing set for titles...\")\n",
    "\n",
    "for test_title in X_test['title']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_test_titles.append(review_to_words(test_title))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_titles}.')\n",
    "        \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20989"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6997"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the \"CountVectorizer\" object, which is scikit-learn's bag of words tool\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 2500,\n",
    "                             min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_titles)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_titles)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array.\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20989, 2500)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6997, 2500)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'able', 'absolute', 'abstract', 'abstractive', 'academia', 'academic', 'academy', 'accelerate', 'accelerates', 'accelerating', 'accepted', 'access', 'accessible', 'according', 'account', 'accounting', 'accuracy', 'accurate', 'accurately', 'achieve', 'acl', 'acm', 'acquisition', 'across', 'act', 'action', 'actions', 'activation', 'active', 'activities', 'activity', 'actually', 'ad', 'adaptation', 'adaptive', 'add', 'added', 'adding', 'address', 'adobe', 'adopt', 'adoption', 'ads', 'advance', 'advanced', 'advancement', 'advancements', 'advances', 'advancing', 'advantage', 'advantages', 'adversarial', 'advertising', 'advice', 'ae', 'affect', 'age', 'agency', 'agent', 'agents', 'agi', 'ago', 'agricultural', 'agriculture', 'ahead', 'ai', 'aid', 'aims', 'aiops', 'air', 'ais', 'al', 'alexa', 'alexnet', 'algebra', 'algo', 'algorithm', 'algorithms', 'alibaba', 'alignment', 'allow', 'allows', 'almost', 'along', 'alphago', 'already', 'also', 'alternative', 'alternatives', 'always', 'amazing', 'amazon', 'amd', 'america', 'amid', 'among', 'amp', 'anaconda', 'analysis', 'analyst', 'analytics', 'analyze', 'analyzing', 'anchor', 'andrew', 'android', 'animal', 'animation', 'anime', 'annotation', 'announced', 'announces', 'announcing', 'anns', 'anomaly', 'another', 'answer', 'answering', 'answers', 'anti', 'antibiotics', 'anybody', 'anyone', 'anything', 'apache', 'api', 'apis', 'app', 'apple', 'application', 'applications', 'applied', 'apply', 'applying', 'appreciate', 'appreciated', 'approach', 'approaches', 'approaching', 'appropriate', 'apps', 'april', 'ar', 'arbitrary', 'architecture', 'architectures', 'arduino', 'area', 'areas', 'arguments', 'arm', 'around', 'array', 'art', 'article', 'articles', 'artifical', 'artificial', 'artificialintelligence', 'artificially', 'arxiv', 'ash', 'asia', 'ask', 'asked', 'assist', 'assistance', 'assistant', 'assistants', 'assisted', 'associated', 'association', 'atari', 'attacks', 'attention', 'attributes', 'auc', 'audio', 'augmentation', 'augmented', 'author', 'authors', 'auto', 'autoencoder', 'autoencoders', 'automate', 'automated', 'automatic', 'automatically', 'automating', 'automation', 'automl', 'automotive', 'autonomous', 'available', 'avatars', 'avoid', 'award', 'aware', 'away', 'awesome', 'aws', 'azure', 'back', 'background', 'backpropagation', 'backward', 'bad', 'baidu', 'balance', 'bank', 'banking', 'banks', 'based', 'baseline', 'basic', 'basics', 'basketball', 'batch', 'battle', 'bayes', 'bayesian', 'bc', 'bd', 'beat', 'beats', 'beautiful', 'beblogging', 'became', 'become', 'becoming', 'begin', 'beginner', 'beginners', 'behave', 'behavior', 'behind', 'believe', 'benchmark', 'benchmarking', 'benchmarks', 'benefit', 'benefits', 'bengio', 'bert', 'besides', 'best', 'beta', 'better', 'beyond', 'bi', 'bias', 'biases', 'big', 'biggest', 'bill', 'billion', 'binary', 'biological', 'biomedical', 'bird', 'bit', 'bitcoin', 'black', 'blender', 'blind', 'block', 'blockchain', 'blog', 'blogs', 'blue', 'board', 'body', 'book', 'books', 'boon', 'boost', 'boosting', 'boosts', 'bot', 'bots', 'bounding', 'box', 'brain', 'brains', 'brand', 'brands', 'break', 'breaking', 'breakthrough', 'breakthroughs', 'breast', 'brief', 'bring', 'bringing', 'brings', 'browser', 'budget', 'build', 'building', 'built', 'bunch', 'business', 'businesses', 'buy', 'cagr', 'calculating', 'call', 'called', 'camera', 'cameras', 'campaign', 'campaigns', 'canada', 'cancer', 'cannot', 'capabilities', 'car', 'card', 'care', 'career', 'carlo', 'cars', 'case', 'cases', 'cat', 'catching', 'categorical', 'category', 'causal', 'cause', 'cc', 'cell', 'cells', 'center', 'centers', 'century', 'ceo', 'certain', 'certificate', 'certification', 'certified', 'chain', 'challenge', 'challenges', 'chance', 'change', 'changed', 'changer', 'changes', 'changing', 'channel', 'channels', 'character', 'characters', 'chat', 'chatbot', 'chatbots', 'cheap', 'cheat', 'check', 'chennai', 'chess', 'child', 'children', 'china', 'chinese', 'chip', 'chips', 'choose', 'choosing', 'chrome', 'cities', 'city', 'claims', 'class', 'classes', 'classical', 'classification', 'classifier', 'classifiers', 'classify', 'classifying', 'cleaning', 'click', 'climate', 'clinical', 'close', 'closer', 'clothing', 'cloud', 'clouds', 'club', 'cluster', 'clustering', 'cmu', 'cnn', 'cnns', 'co', 'coco', 'code', 'codes', 'coding', 'cognitive', 'colab', 'collaboration', 'collaborative', 'collection', 'college', 'collision', 'color', 'com', 'combat', 'combination', 'combining', 'come', 'comes', 'coming', 'command', 'commands', 'comment', 'comments', 'commerce', 'commercial', 'common', 'communication', 'community', 'companies', 'company', 'compare', 'comparing', 'comparison', 'compete', 'competition', 'competitions', 'competitive', 'complete', 'complex', 'complexity', 'component', 'components', 'comprehension', 'comprehensive', 'computation', 'computational', 'compute', 'computer', 'computers', 'computing', 'concept', 'concepts', 'concerns', 'conditional', 'conference', 'conferences', 'confidence', 'confused', 'confusion', 'connected', 'cons', 'conscious', 'consciousness', 'consider', 'considered', 'construction', 'consulting', 'consumer', 'contact', 'content', 'context', 'contextual', 'continuous', 'contract', 'contrastive', 'control', 'controlled', 'controls', 'convergence', 'conversation', 'conversational', 'conversations', 'convert', 'converting', 'convolution', 'convolutional', 'convolutions', 'cool', 'core', 'corona', 'coronavirus', 'corpus', 'correct', 'correction', 'correlation', 'cortex', 'cost', 'could', 'council', 'count', 'countries', 'country', 'course', 'coursera', 'courses', 'cover', 'covid', 'cpu', 'crazy', 'create', 'created', 'creates', 'creating', 'creation', 'creative', 'creators', 'credit', 'crimes', 'crisis', 'critical', 'crm', 'cross', 'crossing', 'crucial', 'cryptocurrency', 'cryptography', 'cs', 'csv', 'ct', 'cube', 'cuda', 'culture', 'curated', 'cure', 'curious', 'current', 'currently', 'curriculum', 'curve', 'curves', 'custom', 'customer', 'customers', 'cutting', 'cv', 'cvpr', 'cyber', 'cybersecurity', 'daily', 'damage', 'dangerous', 'dangers', 'dark', 'dashboard', 'data', 'database', 'dataset', 'datasets', 'date', 'day', 'days', 'dbscan', 'dc', 'de', 'deadline', 'deal', 'dealing', 'death', 'debate', 'decade', 'decentralized', 'decide', 'decision', 'decisions', 'decoder', 'decoding', 'dedicated', 'deep', 'deepfacedrawing', 'deepfake', 'deepfakes', 'deeplearning', 'deepmind', 'defense', 'degree', 'degrees', 'deliver', 'delivering', 'delivery', 'demand', 'demo', 'department', 'deploy', 'deploying', 'deployment', 'depth', 'derivative', 'descent', 'description', 'design', 'designed', 'designing', 'desk', 'desktop', 'details', 'detect', 'detecting', 'detection', 'detector', 'detects', 'determine', 'dev', 'develop', 'developed', 'developer', 'developers', 'developing', 'development', 'developments', 'device', 'devices', 'devops', 'diagnose', 'diagnosis', 'dialogue', 'difference', 'differences', 'different', 'differentiable', 'differential', 'difficult', 'difficulty', 'digit', 'digital', 'dimension', 'dimensional', 'dimensions', 'direction', 'directly', 'director', 'discord', 'discover', 'discovery', 'discrete', 'discussion', 'disease', 'diseases', 'disrupt', 'disrupting', 'disruption', 'disruptive', 'dissertation', 'distance', 'distancing', 'distributed', 'distribution', 'dive', 'dl', 'dm', 'dnn', 'document', 'documentary', 'documentation', 'documents', 'dog', 'dollar', 'domain', 'done', 'double', 'doubt', 'download', 'dqn', 'dr', 'draw', 'dream', 'drive', 'driven', 'driving', 'drone', 'drones', 'drug', 'drugs', 'ds', 'dual', 'dubai', 'due', 'dungeon', 'dynamic', 'dynamics', 'early', 'earn', 'earth', 'easier', 'easiest', 'easily', 'easy', 'ec', 'eccv', 'ecommerce', 'economic', 'economy', 'ecosystem', 'ed', 'edge', 'education', 'educational', 'ee', 'ef', 'effect', 'effective', 'effectively', 'effects', 'efficiency', 'efficient', 'efficiently', 'el', 'electric', 'elon', 'else', 'email', 'embedded', 'embedding', 'embeddings', 'embrace', 'emerging', 'emnlp', 'emotion', 'emotional', 'emotions', 'employee', 'empower', 'en', 'enable', 'enabled', 'enables', 'enabling', 'encoder', 'encoding', 'end', 'energy', 'engagement', 'engine', 'engineer', 'engineering', 'engineers', 'engines', 'english', 'enhance', 'enhanced', 'enough', 'ensure', 'enterprise', 'enterprises', 'entertainment', 'enthusiasts', 'entire', 'entrepreneur', 'entrepreneurs', 'entropy', 'environment', 'environments', 'episode', 'equation', 'equations', 'equivalent', 'era', 'error', 'errors', 'essay', 'essential', 'estimation', 'et', 'etc', 'ethical', 'ethics', 'europe', 'evaluate', 'evaluating', 'evaluation', 'even', 'event', 'events', 'eventually', 'ever', 'every', 'everyone', 'everything', 'evolution', 'evolutionary', 'evolve', 'evolving', 'exactly', 'exam', 'example', 'examples', 'excellent', 'exchange', 'excited', 'exciting', 'exist', 'existing', 'expand', 'expanding', 'expect', 'expected', 'experience', 'experiences', 'experiment', 'experimental', 'experiments', 'expert', 'experts', 'explain', 'explainable', 'explained', 'explaining', 'explains', 'explanation', 'exploration', 'explore', 'exploring', 'expressions', 'extended', 'extension', 'extensive', 'extract', 'extracting', 'extraction', 'eye', 'eyes', 'face', 'facebook', 'faces', 'facial', 'fact', 'factor', 'factorization', 'factors', 'facts', 'fail', 'failure', 'fake', 'fall', 'famous', 'far', 'farming', 'fashion', 'fast', 'faster', 'fastest', 'favorite', 'favourite', 'fb', 'fe', 'fear', 'feature', 'features', 'federated', 'feed', 'feedback', 'feel', 'fiction', 'fidelity', 'field', 'fields', 'fight', 'fighting', 'figure', 'file', 'files', 'fill', 'filling', 'film', 'filter', 'filtering', 'final', 'finally', 'finance', 'financial', 'find', 'finding', 'finds', 'fine', 'fintech', 'first', 'fit', 'fitting', 'five', 'fix', 'flask', 'flow', 'flower', 'flows', 'focus', 'focused', 'follow', 'following', 'food', 'force', 'forecast', 'forecasting', 'forecasts', 'forest', 'form', 'format', 'forward', 'found', 'foundations', 'founder', 'founders', 'four', 'fourth', 'fps', 'frame', 'framework', 'frameworks', 'fraud', 'free', 'frequency', 'fridman', 'friend', 'friendly', 'front', 'frontier', 'fuel', 'full', 'fully', 'fun', 'function', 'functional', 'functions', 'fund', 'fundamental', 'funding', 'funny', 'future', 'gain', 'gains', 'game', 'games', 'gaming', 'gan', 'gans', 'gap', 'gathering', 'gaussian', 'gb', 'gender', 'general', 'generalization', 'generate', 'generated', 'generates', 'generating', 'generation', 'generative', 'generator', 'genetic', 'geometric', 'gesture', 'get', 'gets', 'getting', 'giant', 'giants', 'github', 'give', 'given', 'gives', 'global', 'go', 'goal', 'goals', 'god', 'goes', 'gofounders', 'going', 'good', 'google', 'got', 'governance', 'government', 'gpt', 'gpu', 'gpus', 'grad', 'gradient', 'graduate', 'grammar', 'graph', 'graphical', 'graphics', 'graphs', 'great', 'greater', 'greatest', 'greedy', 'ground', 'group', 'groups', 'grow', 'growing', 'growth', 'gt', 'gtx', 'guess', 'guidance', 'guide', 'guided', 'guidelines', 'guy', 'guys', 'gym', 'hackathon', 'hackers', 'hacks', 'half', 'hand', 'handle', 'handling', 'hands', 'handwritten', 'happen', 'happening', 'happens', 'happy', 'hard', 'hardware', 'harnessing', 'harry', 'harvard', 'head', 'health', 'healthcare', 'hear', 'heard', 'heart', 'hello', 'help', 'helped', 'helpful', 'helping', 'helps', 'hey', 'hi', 'hidden', 'hide', 'hierarchical', 'high', 'higher', 'highlight', 'highlights', 'highly', 'hinton', 'hiring', 'history', 'hit', 'hold', 'holy', 'home', 'hope', 'hospitals', 'hosting', 'hot', 'hours', 'house', 'hr', 'html', 'http', 'https', 'huawei', 'huge', 'human', 'humanity', 'humanoid', 'humans', 'hybrid', 'hyderabad', 'hype', 'hyper', 'hyperparameter', 'hypothesis', 'ia', 'ibm', 'iclr', 'icml', 'icymi', 'id', 'idea', 'ideas', 'identification', 'identifies', 'identify', 'identifying', 'identity', 'ieee', 'ii', 'iii', 'iit', 'ijcai', 'im', 'image', 'imagenet', 'images', 'imagine', 'imaging', 'imbalanced', 'imitation', 'impact', 'impacted', 'impacting', 'impacts', 'imperative', 'implement', 'implementation', 'implementations', 'implemented', 'implementing', 'implications', 'implicit', 'importance', 'important', 'impressive', 'improve', 'improved', 'improvements', 'improves', 'improving', 'inc', 'include', 'including', 'increase', 'increasing', 'incredible', 'index', 'india', 'indian', 'individuals', 'industrial', 'industries', 'industry', 'inference', 'influence', 'info', 'infographic', 'infographics', 'information', 'infrastructure', 'initiative', 'initiatives', 'innovation', 'innovations', 'innovative', 'inpainting', 'input', 'inputs', 'inside', 'insight', 'insights', 'inspired', 'instagram', 'install', 'instance', 'instead', 'institute', 'insurance', 'integrated', 'integrating', 'integration', 'intel', 'inteligencia', 'intelligence', 'intelligent', 'interact', 'interaction', 'interactions', 'interactive', 'interested', 'interesting', 'interface', 'international', 'internet', 'internship', 'interpret', 'interpretable', 'interpreting', 'interview', 'interviews', 'intro', 'introduce', 'introduces', 'introducing', 'introduction', 'intuitive', 'invariant', 'invest', 'investing', 'investment', 'investments', 'invoice', 'io', 'ios', 'iot', 'issue', 'issues', 'jack', 'japanese', 'java', 'javascript', 'jax', 'jetson', 'job', 'jobs', 'joe', 'join', 'joining', 'joins', 'joint', 'jokes', 'journal', 'journey', 'js', 'jukebox', 'julia', 'july', 'june', 'jupyter', 'kafka', 'kaggle', 'keep', 'keeping', 'keras', 'kernel', 'key', 'kids', 'kill', 'killer', 'kind', 'knn', 'know', 'knowledge', 'known', 'knows', 'la', 'lab', 'label', 'labeling', 'labelling', 'labels', 'labs', 'land', 'landscape', 'language', 'languages', 'laptop', 'large', 'largest', 'last', 'latent', 'latest', 'launch', 'launched', 'launches', 'launching', 'law', 'layer', 'layers', 'lead', 'leader', 'leaders', 'leading', 'leads', 'learn', 'learned', 'learners', 'learning', 'learns', 'least', 'lecture', 'lectures', 'lecun', 'left', 'legal', 'length', 'less', 'lessons', 'let', 'letter', 'level', 'levels', 'leverage', 'leveraging', 'lex', 'li', 'libraries', 'library', 'license', 'lidar', 'lies', 'life', 'light', 'lightning', 'lightweight', 'like', 'likelihood', 'likely', 'limited', 'limits', 'line', 'linear', 'lines', 'link', 'linkedin', 'list', 'listening', 'lite', 'literature', 'little', 'live', 'lives', 'livestream', 'local', 'localization', 'log', 'logic', 'logistic', 'logistics', 'london', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loop', 'los', 'loss', 'lot', 'love', 'low', 'lstm', 'lt', 'lyrics', 'macbook', 'machine', 'machinelearning', 'machines', 'made', 'magazine', 'magic', 'main', 'maintenance', 'major', 'make', 'makes', 'making', 'man', 'manage', 'managed', 'management', 'managing', 'manipulation', 'manner', 'manual', 'manufacturing', 'many', 'map', 'mapping', 'maps', 'march', 'mark', 'market', 'marketers', 'marketing', 'marketplace', 'markets', 'marketsandmarkets', 'markov', 'mars', 'mask', 'masked', 'mass', 'massive', 'master', 'masters', 'match', 'matching', 'material', 'materials', 'math', 'mathematical', 'mathematics', 'mathias', 'maths', 'matrix', 'matter', 'matters', 'max', 'maximize', 'maximum', 'may', 'maybe', 'mean', 'meaning', 'means', 'measure', 'measuring', 'mechanism', 'mechanisms', 'media', 'medical', 'medicine', 'medium', 'meena', 'meet', 'meets', 'meetup', 'meme', 'memory', 'mental', 'message', 'messenger', 'meta', 'method', 'methods', 'metric', 'metrics', 'microsoft', 'might', 'miko', 'military', 'million', 'millions', 'min', 'mind', 'minecraft', 'mini', 'minimal', 'mining', 'minute', 'minutes', 'mirror', 'missing', 'mistake', 'mistakes', 'mit', 'mixed', 'mixture', 'ml', 'mlops', 'mnist', 'mobile', 'mode', 'model', 'modeling', 'modelling', 'models', 'modern', 'module', 'money', 'monitor', 'monitoring', 'monte', 'months', 'motion', 'move', 'movement', 'movements', 'movie', 'movies', 'moving', 'mr', 'ms', 'much', 'mufareh', 'multi', 'multiclass', 'multilingual', 'multimodal', 'multiple', 'multivariate', 'music', 'musk', 'must', 'myths', 'naive', 'name', 'named', 'names', 'nas', 'nation', 'national', 'natural', 'nature', 'nd', 'near', 'nearest', 'necessary', 'need', 'needed', 'needs', 'negative', 'ner', 'net', 'nets', 'network', 'networks', 'neural', 'neuralink', 'neurips', 'neuroevolution', 'neuron', 'neurons', 'neuroscience', 'never', 'new', 'newbie', 'news', 'next', 'ng', 'nice', 'nlp', 'nn', 'node', 'noise', 'non', 'noob', 'normal', 'normalization', 'normalizing', 'notebook', 'notebooks', 'notes', 'nothing', 'novel', 'number', 'numbers', 'numpy', 'nvidia', 'object', 'objects', 'ocr', 'offer', 'offering', 'offers', 'office', 'official', 'offline', 'often', 'oil', 'old', 'olportal', 'one', 'ones', 'online', 'onnx', 'onpassive', 'open', 'openai', 'opencv', 'opening', 'operation', 'operations', 'operators', 'opinion', 'opinions', 'opportunities', 'opportunity', 'optical', 'optimal', 'optimization', 'optimize', 'optimizer', 'optimizing', 'option', 'options', 'oral', 'order', 'org', 'organization', 'organizations', 'organize', 'os', 'others', 'outbreak', 'outcome', 'outcomes', 'outlier', 'outlook', 'output', 'outputs', 'outside', 'outstanding', 'overfitting', 'overview', 'pac', 'package', 'page', 'paid', 'pandas', 'pandemic', 'paper', 'papers', 'parallel', 'parallelism', 'parameter', 'parameters', 'part', 'particular', 'partner', 'partners', 'partnership', 'parts', 'passing', 'past', 'path', 'patient', 'patients', 'pattern', 'patterns', 'paving', 'pay', 'paying', 'payments', 'pc', 'pca', 'pdf', 'peer', 'people', 'per', 'perception', 'perceptron', 'perfect', 'perform', 'performance', 'performing', 'person', 'personal', 'personalized', 'perspective', 'phd', 'phone', 'photo', 'photorealistic', 'photos', 'physical', 'physics', 'pi', 'picture', 'pictures', 'pifuhd', 'pipeline', 'pix', 'pixel', 'pixels', 'place', 'places', 'plan', 'planet', 'planning', 'plans', 'platform', 'platforms', 'play', 'player', 'players', 'playing', 'plays', 'please', 'plot', 'pm', 'poc', 'podcast', 'podcasts', 'point', 'points', 'policies', 'policy', 'pooling', 'popular', 'portrait', 'portraits', 'pose', 'position', 'positive', 'possibilities', 'possibility', 'possible', 'post', 'posted', 'posts', 'potential', 'power', 'powered', 'powerful', 'powering', 'practical', 'practice', 'practices', 'pre', 'precision', 'predict', 'predicting', 'prediction', 'predictions', 'predictive', 'predictor', 'predicts', 'preparation', 'prepare', 'prepared', 'preparing', 'preprocessing', 'presence', 'present', 'presentation', 'press', 'pretrained', 'pretty', 'prevent', 'price', 'prices', 'pricing', 'primer', 'principles', 'prior', 'privacy', 'private', 'pro', 'probabilistic', 'probabilities', 'probability', 'problem', 'problems', 'process', 'processes', 'processing', 'processor', 'produce', 'product', 'production', 'productivity', 'products', 'professional', 'professionals', 'professor', 'profit', 'profitable', 'program', 'programming', 'programs', 'progress', 'progression', 'progressive', 'project', 'projects', 'promising', 'proof', 'propagation', 'proper', 'proposal', 'pros', 'prospects', 'protect', 'protein', 'proud', 'proven', 'provide', 'provider', 'provides', 'pseudo', 'psychology', 'pt', 'public', 'publish', 'published', 'publishing', 'pulse', 'purpose', 'purposes', 'pursue', 'pursuing', 'put', 'pycaret', 'python', 'pytorch', 'quality', 'quantum', 'quarantine', 'query', 'question', 'questions', 'quick', 'quickly', 'quite', 'quiz', 'race', 'racing', 'rainfall', 'raises', 'random', 'range', 'rank', 'ranking', 'rapid', 'rapidly', 'raspberry', 'rate', 'rather', 'raval', 'ray', 'rcnn', 'reach', 'read', 'reading', 'ready', 'real', 'realistic', 'reality', 'really', 'reason', 'reasoning', 'reasons', 'recall', 'recent', 'recently', 'recognition', 'recognize', 'recognizing', 'recommend', 'recommendation', 'recommendations', 'recommended', 'recommender', 'reconstruction', 'reconstructions', 'record', 'records', 'recruiting', 'recruitment', 'recurrent', 'reddit', 'reduce', 'reducing', 'reduction', 'reference', 'regarding', 'region', 'regression', 'regular', 'regularization', 'reinforcement', 'related', 'relationship', 'relationships', 'release', 'released', 'releases', 'relevant', 'religion', 'relu', 'remember', 'reminder', 'remote', 'removal', 'remove', 'rendering', 'replace', 'replay', 'replika', 'repo', 'report', 'reports', 'repository', 'representation', 'representations', 'request', 'require', 'required', 'requirement', 'requirements', 'requires', 'research', 'researcher', 'researchers', 'reshape', 'reshaping', 'residency', 'resnet', 'resolution', 'resource', 'resources', 'response', 'responses', 'responsible', 'restoration', 'restore', 'results', 'resume', 'retail', 'rethinking', 'revealed', 'revenue', 'reverse', 'review', 'reviewing', 'reviews', 'revolution', 'revolutionary', 'revolutionize', 'revolutionizing', 'reward', 'rich', 'ride', 'right', 'rights', 'rise', 'rising', 'risk', 'risks', 'rl', 'rnn', 'rnns', 'road', 'roadmap', 'robot', 'robotic', 'robotics', 'robots', 'robust', 'robustness', 'roc', 'rock', 'roi', 'role', 'roles', 'round', 'rpa', 'rtx', 'rule', 'rules', 'run', 'running', 'safe', 'safety', 'said', 'sales', 'sample', 'samples', 'sampling', 'samsung', 'sap', 'save', 'say', 'says', 'scalable', 'scale', 'scaling', 'scan', 'scans', 'scared', 'scary', 'scene', 'schmidhuber', 'school', 'schools', 'sci', 'science', 'scientific', 'scientist', 'scientists', 'scikit', 'score', 'scores', 'scraping', 'scratch', 'screen', 'script', 'search', 'searching', 'second', 'seconds', 'secret', 'secrets', 'sector', 'sectors', 'secure', 'security', 'see', 'seed', 'seeing', 'seek', 'seeking', 'seem', 'seems', 'seen', 'segmentation', 'selection', 'self', 'selfie', 'semantic', 'semi', 'senior', 'sense', 'sensors', 'sentence', 'sentiment', 'seo', 'seq', 'sequence', 'sequences', 'sequential', 'series', 'serious', 'serve', 'server', 'service', 'services', 'serving', 'session', 'set', 'sets', 'setting', 'setup', 'seven', 'several', 'sexy', 'shape', 'shaping', 'share', 'shared', 'sharing', 'sheet', 'shift', 'short', 'shot', 'show', 'shows', 'side', 'sigmoid', 'sign', 'signal', 'signals', 'significant', 'silicon', 'similar', 'similarity', 'simple', 'simply', 'simulated', 'simulation', 'since', 'single', 'singularity', 'singularitynet', 'sir', 'siraj', 'siri', 'site', 'sites', 'six', 'size', 'sizes', 'sketch', 'sketches', 'skill', 'skills', 'skin', 'sklearn', 'slack', 'sleep', 'small', 'smart', 'smarter', 'smartphone', 'snake', 'soar', 'social', 'society', 'soft', 'software', 'solid', 'solution', 'solutions', 'solve', 'solver', 'solves', 'solving', 'somebody', 'someone', 'something', 'sometimes', 'song', 'songs', 'soon', 'sophia', 'sort', 'sota', 'sound', 'sounds', 'source', 'sourced', 'sources', 'sourcing', 'space', 'spark', 'sparse', 'spatial', 'speaker', 'special', 'specialization', 'specific', 'specifically', 'speech', 'speed', 'split', 'sports', 'spot', 'spread', 'sql', 'st', 'stable', 'stack', 'stage', 'standard', 'stanford', 'starcraft', 'start', 'started', 'starting', 'startup', 'startups', 'state', 'statement', 'states', 'statistical', 'statistics', 'stats', 'status', 'stay', 'steal', 'step', 'stephen', 'steps', 'still', 'stochastic', 'stock', 'stocks', 'stop', 'store', 'stories', 'story', 'strategic', 'strategies', 'strategy', 'stream', 'streaming', 'street', 'strong', 'structure', 'structured', 'structures', 'student', 'students', 'studies', 'studio', 'study', 'studying', 'stuff', 'style', 'stylegan', 'sub', 'subject', 'submission', 'submit', 'subreddit', 'success', 'successful', 'successfully', 'suggest', 'suggestion', 'suggestions', 'summarization', 'summarizing', 'summary', 'summer', 'summit', 'super', 'supercharge', 'supercomputer', 'supervised', 'supply', 'support', 'sure', 'surprising', 'surveillance', 'survey', 'survive', 'svm', 'swaayatt', 'swapping', 'swift', 'symbolic', 'symptoms', 'synthesis', 'synthetic', 'system', 'systems', 'table', 'tabular', 'tackle', 'tagging', 'take', 'taken', 'takes', 'taking', 'talent', 'talk', 'talking', 'talks', 'target', 'task', 'tasks', 'taught', 'teach', 'teaching', 'team', 'teams', 'tech', 'technical', 'technique', 'techniques', 'technologies', 'technology', 'telecom', 'telecommunication', 'telecommunications', 'telegram', 'tell', 'temporal', 'ten', 'tennis', 'tensor', 'tensorflow', 'term', 'terms', 'tesla', 'test', 'testing', 'tests', 'tetris', 'text', 'texts', 'textual', 'tf', 'th', 'thank', 'thanks', 'theoretical', 'theory', 'thesis', 'thing', 'things', 'think', 'thinking', 'thought', 'thoughts', 'thousands', 'thread', 'threat', 'three', 'ti', 'time', 'timeline', 'times', 'tiny', 'tips', 'title', 'today', 'together', 'tomorrow', 'tool', 'toolbox', 'toolkit', 'tools', 'top', 'topic', 'topics', 'tops', 'touch', 'toward', 'towards', 'tpu', 'track', 'tracking', 'trade', 'trading', 'traditional', 'traffic', 'train', 'trained', 'training', 'transcription', 'transfer', 'transform', 'transformation', 'transformed', 'transformer', 'transformers', 'transforming', 'translate', 'translation', 'transportation', 'travel', 'tree', 'trees', 'trend', 'trending', 'trends', 'tricks', 'tried', 'true', 'truly', 'trump', 'trust', 'truth', 'try', 'trying', 'ttnet', 'tts', 'tune', 'tuning', 'turing', 'turn', 'turning', 'turns', 'tutorial', 'tutorials', 'tv', 'tweets', 'twin', 'twitter', 'two', 'type', 'types', 'uber', 'udacity', 'ui', 'uk', 'ultimate', 'un', 'uncertainty', 'undergrad', 'undergraduate', 'understand', 'understanding', 'unemployment', 'unet', 'unique', 'unit', 'united', 'unity', 'universal', 'universe', 'universities', 'university', 'unsupervised', 'unveils', 'upcoming', 'update', 'updated', 'updates', 'upload', 'ups', 'upscaling', 'urgent', 'us', 'usa', 'usage', 'usd', 'use', 'used', 'useful', 'user', 'users', 'uses', 'using', 'ux', 'vaccine', 'vae', 'validation', 'value', 'values', 'variable', 'variables', 'variance', 'variational', 'various', 'vec', 'vector', 'vectors', 'vehicle', 'vehicles', 'verification', 'version', 'versus', 'via', 'video', 'videos', 'view', 'virtual', 'virus', 'vision', 'visual', 'visualization', 'visualize', 'visualizing', 'voice', 'vr', 'vs', 'walk', 'want', 'wanted', 'wants', 'war', 'warning', 'waste', 'watch', 'water', 'watson', 'wave', 'way', 'wayr', 'ways', 'weather', 'web', 'webinar', 'website', 'websites', 'webtunix', 'week', 'weekly', 'weeks', 'weight', 'weights', 'weird', 'welcome', 'well', 'whatsapp', 'whether', 'white', 'whole', 'wholly', 'wide', 'win', 'windows', 'winning', 'wise', 'within', 'without', 'women', 'wonder', 'wondering', 'wonders', 'word', 'words', 'work', 'worked', 'workers', 'workflow', 'workflows', 'workforce', 'working', 'workplace', 'works', 'workshop', 'world', 'worldwide', 'worse', 'worth', 'would', 'write', 'writing', 'written', 'wrong', 'wrote', 'www', 'xgboost', 'yann', 'year', 'years', 'yes', 'yet', 'yolo', 'yolov', 'york', 'yoshua', 'youtube', 'zero', 'zoom']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import logistic regression.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', solver = 'liblinear', C = .10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8321501739006146"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174932113763042"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00563485,  0.00382158, -0.65031521, ..., -0.35071622,\n",
       "         0.74403866, -0.62460665]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model one LASSO Logistic Regression with alpha = 10 gets a train score of .83 and a test score of .82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
