{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have to install GetOldTweet3    \n",
    "https://pypi.org/project/GetOldTweets3/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tweepy as tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Grab Date, Tweet, & Username For the Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownloadTweets(SinceDate, UntilDate, Query):\n",
    "    '''\n",
    "    Downloads all tweets from a certain month in three sessions in order to avoid sending too many requests. \n",
    "    Date format = 'yyyy-mm-dd'. \n",
    "    '''\n",
    "    since = datetime.strptime(SinceDate, '%Y-%m-%d')\n",
    "    until= datetime.strptime(UntilDate, '%Y-%m-%d')\n",
    "    tenth = since + timedelta(days = 10)\n",
    "    twentieth = since + timedelta(days=20)\n",
    "    \n",
    "    print ('starting first download')\n",
    "    first = got.manager.TweetCriteria().setQuerySearch(Query).setSince(since.strftime('%Y-%m-%d')).setUntil(tenth.strftime('%Y-%m-%d'))\n",
    "    firstdownload = got.manager.TweetManager.getTweets(first)\n",
    "    firstlist=[[tweet.date, tweet.text, tweet.username] for tweet in firstdownload]\n",
    "    \n",
    "    df_1 = pd.DataFrame.from_records(firstlist, columns = [\"date\", \"tweet\", \"user\"])\n",
    "#     df_1.to_csv(\"%s_1.csv\" % SinceDate)\n",
    "    \n",
    "    time.sleep(600)\n",
    "    \n",
    "    print ('starting second download')\n",
    "    second = got.manager.TweetCriteria().setQuerySearch(Query).setSince(tenth.strftime('%Y-%m-%d')).setUntil(twentieth.strftime('%Y-%m-%d'))\n",
    "    seconddownload = got.manager.TweetManager.getTweets(second)\n",
    "    secondlist=[[tweet.date, tweet.text, tweet.username] for tweet in seconddownload]\n",
    "    \n",
    "    df_2 = pd.DataFrame.from_records(secondlist, columns = [\"date\", \"tweet\", \"user\"])\n",
    "#     df_2.to_csv(\"%s_2.csv\" % SinceDate)\n",
    "    \n",
    "    time.sleep(600)\n",
    "    \n",
    "    print ('starting third download')\n",
    "    print('='*20)\n",
    "    third = got.manager.TweetCriteria().setQuerySearch(Query).setSince(twentieth.strftime('%Y-%m-%d')).setUntil(until.strftime('%Y-%m-%d'))\n",
    "    thirddownload = got.manager.TweetManager.getTweets(third)\n",
    "    thirdlist=[[tweet.date, tweet.text, tweet.username] for tweet in thirddownload]\n",
    "    \n",
    "    df_3 = pd.DataFrame.from_records(thirdlist, columns = [\"date\", \"tweet\", \"user\"])\n",
    "#     df_3.to_csv(\"%s_3.csv\" % SinceDate)\n",
    "    \n",
    "    df=pd.concat([df_1,df_2,df_3])\n",
    "#     df.to_csv(\"%s.csv\" % SinceDate)\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pull In Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Power Went Out\n",
    "jan_fin_tweets_mpwo =  DownloadTweets('2020-01-01', '2020-01-31', 'my power went out')\n",
    "time.sleep(100)\n",
    "feb_fin_tweets_mpwo =  DownloadTweets('2020-02-01', '2020-02-29', 'my power went out')\n",
    "time.sleep(100)\n",
    "mar_fin_tweets_mpwo =  DownloadTweets('2020-03-01', '2020-03-31', 'my power went out')\n",
    "time.sleep(100)\n",
    "apr_fin_tweets_mpwo =  DownloadTweets('2020-04-01', '2020-04-30', 'my power went out')\n",
    "time.sleep(100)\n",
    "may_fin_tweets_mpwo =  DownloadTweets('2020-05-01', '2020-05-31', 'my power went out')\n",
    "time.sleep(100)\n",
    "jun_fin_tweets_mpwo =  DownloadTweets('2020-06-01', '2020-06-30', 'my power went out')\n",
    "time.sleep(100)\n",
    "jul_fin_tweets_mpwo =  DownloadTweets('2020-07-01', '2020-07-31', 'my power went out')\n",
    "time.sleep(100)\n",
    "aug_fin_tweets_mpwo =  DownloadTweets('2020-08-01', '2020-08-31', 'my power went out')\n",
    "time.sleep(100)\n",
    "\n",
    "# I lost power\n",
    "jan_fin_tweets_ilp =  DownloadTweets('2020-01-01', '2020-01-31', 'i lost power')\n",
    "time.sleep(100)\n",
    "feb_fin_tweets_ilp =  DownloadTweets('2020-02-01', '2020-02-29', 'i lost power')\n",
    "time.sleep(100)\n",
    "mar_fin_tweets_ilp =  DownloadTweets('2020-03-01', '2020-03-31', 'i lost power')\n",
    "time.sleep(100)\n",
    "apr_fin_tweets_ilp =  DownloadTweets('2020-04-01', '2020-04-30', 'i lost power')\n",
    "time.sleep(100)\n",
    "may_fin_tweets_ilp =  DownloadTweets('2020-05-01', '2020-05-31', 'i lost power')\n",
    "time.sleep(100)\n",
    "jun_fin_tweets_ilp =  DownloadTweets('2020-06-01', '2020-06-30', 'i lost power')\n",
    "time.sleep(100)\n",
    "jul_fin_tweets_ilp =  DownloadTweets('2020-07-01', '2020-07-31', 'i lost power')\n",
    "time.sleep(100)\n",
    "aug_fin_tweets_ilp =  DownloadTweets('2020-08-01', '2020-08-31', 'i lost power')\n",
    "tim.sleep(100)\n",
    "\n",
    "# There is a power outage\n",
    "jan_fin_tweets_tpo =  DownloadTweets('2020-01-01', '2020-01-31', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "feb_fin_tweets_tpo =  DownloadTweets('2020-02-01', '2020-02-29', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "mar_fin_tweets_tpo =  DownloadTweets('2020-03-01', '2020-03-31', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "apr_fin_tweets_tpo =  DownloadTweets('2020-04-01', '2020-04-30', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "may_fin_tweets_tpo =  DownloadTweets('2020-05-01', '2020-05-31', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "jun_fin_tweets_tpo =  DownloadTweets('2020-06-01', '2020-06-30', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "jul_fin_tweets_tpo =  DownloadTweets('2020-07-01', '2020-07-31', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "aug_fin_tweets_tpo =  DownloadTweets('2020-08-01', '2020-08-31', 'there is a power outage')\n",
    "time.sleep(100)\n",
    "\n",
    "# I have no power\n",
    "jan_fin_tweets_ihnp =  DownloadTweets('2020-01-01', '2020-01-31', 'I have no power')\n",
    "time.sleep(100)\n",
    "feb_fin_tweets_ihnp =  DownloadTweets('2020-02-01', '2020-02-29', 'I have no power')\n",
    "time.sleep(100)\n",
    "mar_fin_tweets_ihnp =  DownloadTweets('2020-03-01', '2020-03-31', 'I have no power')\n",
    "time.sleep(100)\n",
    "apr_fin_tweets_ihnp =  DownloadTweets('2020-04-01', '2020-04-30', 'I have no power')\n",
    "time.sleep(100)\n",
    "may_fin_tweets_ihnp =  DownloadTweets('2020-05-01', '2020-05-31', 'I have no power')\n",
    "time.sleep(100)\n",
    "jun_fin_tweets_ihnp =  DownloadTweets('2020-06-01', '2020-06-30', 'I have no power')\n",
    "time.sleep(100)\n",
    "jul_fin_tweets_ihnp =  DownloadTweets('2020-07-01', '2020-07-31', 'I have no power')\n",
    "time.sleep(100)\n",
    "aug_fin_tweets_ihnp =  DownloadTweets('2020-08-01', '2020-08-31', 'I have no power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Make Combined DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpwo_final_df = jan_fin_tweets_mpwo.append([feb_fin_tweets_mpwo, mar_fin_tweets_mpwo, apr_fin_tweets_mpwo, \n",
    "                                            may_fin_tweets_mpwo, jun_fin_tweets_mpwo, jul_fin_tweets_mpwo, aug_fin_tweets_mpwo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilp_final_df = jan_fin_tweets_ilp.append([feb_fin_tweets_ilp, mar_fin_tweets_ilp, apr_fin_tweets_ilp, \n",
    "                                            may_fin_tweets_ilp, jun_fin_tweets_ilp, jul_fin_tweets_ilp, aug_fin_tweets_ilp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpo_final_df = jan_fin_tweets_tpo.append([feb_fin_tweets_tpo, mar_fin_tweets_tpo, apr_fin_tweets_tpo, \n",
    "                                            may_fin_tweets_tpo, jun_fin_tweets_tpo, jul_fin_tweets_tpo, aug_fin_tweets_tpo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihnp_final_df = jan_fin_tweets_ihnp.append([feb_fin_tweets_ihnp, mar_fin_tweets_ihnp, apr_fin_tweets_ihnp, \n",
    "                                            may_fin_tweets_ihnp, jun_fin_tweets_ihnp, jul_fin_tweets_ihnp, aug_fin_tweets_ihnp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mpwo_final_df.shape)\n",
    "print(ilp_final_df.shape)\n",
    "print(tpo_final_df.shape)\n",
    "print(ihnp_final_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_processing = mpwo_final_df.append([ilp_final_df,\n",
    "                                             tpo_final_df, # This one will be nathans\n",
    "                                             ihnp_final_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ready_for_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_processing.to_csv('ready_for_processing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aidan = pd.read_csv('mpwo_final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55648, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aidan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "david = pd.read_csv('DLee_final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45012, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "david.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nathan = pd.read_csv('nathan_final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3183, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nathan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill = pd.read_csv('bill_final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh it's forever on my mind! GOP do nothing but whine & cry about their loss of power! In the face of so many precious human lives lost, is the weakest most demonic thing I've ever witnessed! I wasn't alive for Nazi atrocities' so this, is that to me! \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill['tweet'][87454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87459, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = aidan.append([david, nathan, bill])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191302, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Adding Random Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n",
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n",
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n",
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n",
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n",
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n",
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n",
      "starting first download\n",
      "starting second download\n",
      "starting third download\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "jan_fin_tweets_wwos =  DownloadTweets('2020-03-01', '2020-03-31', 'we went out shopping')\n",
    "time.sleep(100)\n",
    "feb_fin_tweets_wwos =  DownloadTweets('2020-02-01', '2020-02-29', 'we went out shopping')\n",
    "time.sleep(100)\n",
    "mar_fin_tweets_wwos =  DownloadTweets('2020-03-01', '2020-03-31', 'we went out shopping')\n",
    "time.sleep(100)\n",
    "apr_fin_tweets_wwos =  DownloadTweets('2020-04-01', '2020-04-30', 'we went out shopping')\n",
    "time.sleep(100)\n",
    "may_fin_tweets_wwos =  DownloadTweets('2020-05-01', '2020-05-31', 'we went out shopping')\n",
    "time.sleep(100)\n",
    "jun_fin_tweets_wwos =  DownloadTweets('2020-06-01', '2020-06-30', 'we went out shopping')\n",
    "time.sleep(100)\n",
    "jul_fin_tweets_wwos =  DownloadTweets('2020-07-01', '2020-07-31', 'we went out shopping')\n",
    "time.sleep(100)\n",
    "aug_fin_tweets_wwos =  DownloadTweets('2020-08-01', '2020-08-31', 'we went out shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-10 23:30:05+00:00</td>\n",
       "      <td>I went shopping for my office today, we were o...</td>\n",
       "      <td>LynnNickey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-03 13:34:34+00:00</td>\n",
       "      <td>They were once the top NBA players, so find ou...</td>\n",
       "      <td>BUZZNET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-10 22:56:29+00:00</td>\n",
       "      <td>Went out shopping at HyVee. Was proud to see #...</td>\n",
       "      <td>burginam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-10 22:47:09+00:00</td>\n",
       "      <td>So we just came back from grocery shopping at ...</td>\n",
       "      <td>COACHELLAEVENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-10 22:42:12+00:00</td>\n",
       "      <td>It's really stupid Craig. We went to Costco a ...</td>\n",
       "      <td>SherrySwinders2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>2020-03-21 01:15:14+00:00</td>\n",
       "      <td>day 3: not insane yet! went grocery shopping c...</td>\n",
       "      <td>PR3TTYDARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>2020-03-21 00:57:09+00:00</td>\n",
       "      <td>my husbands parents went grocery shopping and ...</td>\n",
       "      <td>ihatelaika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2020-03-21 00:25:36+00:00</td>\n",
       "      <td>Usually, on the few occasions that I go food s...</td>\n",
       "      <td>LPGLDN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2020-03-21 00:03:17+00:00</td>\n",
       "      <td>Sorry I missed the deadline but we just got ba...</td>\n",
       "      <td>esoxbox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2020-03-21 00:02:37+00:00</td>\n",
       "      <td>My mate went nappy hunting for 3 days straight...</td>\n",
       "      <td>frankiegraulund</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2063 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date  \\\n",
       "0   2020-03-10 23:30:05+00:00   \n",
       "1   2020-09-03 13:34:34+00:00   \n",
       "2   2020-03-10 22:56:29+00:00   \n",
       "3   2020-03-10 22:47:09+00:00   \n",
       "4   2020-03-10 22:42:12+00:00   \n",
       "..                        ...   \n",
       "697 2020-03-21 01:15:14+00:00   \n",
       "698 2020-03-21 00:57:09+00:00   \n",
       "699 2020-03-21 00:25:36+00:00   \n",
       "700 2020-03-21 00:03:17+00:00   \n",
       "701 2020-03-21 00:02:37+00:00   \n",
       "\n",
       "                                                 tweet             user  \n",
       "0    I went shopping for my office today, we were o...       LynnNickey  \n",
       "1    They were once the top NBA players, so find ou...          BUZZNET  \n",
       "2    Went out shopping at HyVee. Was proud to see #...         burginam  \n",
       "3    So we just came back from grocery shopping at ...  COACHELLAEVENTS  \n",
       "4    It's really stupid Craig. We went to Costco a ...  SherrySwinders2  \n",
       "..                                                 ...              ...  \n",
       "697  day 3: not insane yet! went grocery shopping c...       PR3TTYDARK  \n",
       "698  my husbands parents went grocery shopping and ...       ihatelaika  \n",
       "699  Usually, on the few occasions that I go food s...           LPGLDN  \n",
       "700  Sorry I missed the deadline but we just got ba...          esoxbox  \n",
       "701  My mate went nappy hunting for 3 days straight...  frankiegraulund  \n",
       "\n",
       "[2063 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mar_fin_tweets_wwos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = jan_fin_tweets_wwos.append([feb_fin_tweets_wwos, mar_fin_tweets_wwos, apr_fin_tweets_wwos, may_fin_tweets_wwos,\n",
    "                                       jun_fin_tweets_wwos, jul_fin_tweets_wwos, aug_fin_tweets_wwos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words.to_csv(\"we_went_shopping.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
